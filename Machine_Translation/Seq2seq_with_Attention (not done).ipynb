{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,LSTM,GRU,Dense,Embedding,Bidirectional,RepeatVector\n",
    "from keras.layers import Concatenate,Activation, Add ,Dot,Lambda, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(K.tensorflow_backend._get_available_gpus())>0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM\n",
    "    from keras.layers import CuDNNGRU as GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "EPOCHS=5\n",
    "LATENT_DIM=256\n",
    "NUM_EXAMPLES=1000\n",
    "FETC_EXAMPLES=200000\n",
    "MAX_NUM_WORDS=40000\n",
    "EMBEDDING_DIM=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \"tur.txt\"\n",
    "temporal_encoder_data=[]\n",
    "temporal_decoder_data=[]\n",
    "\n",
    "t=0\n",
    "\n",
    "for line in open(\"tur.txt\"):\n",
    "    t+=1\n",
    "    \n",
    "    if t > FETC_EXAMPLES:\n",
    "        break\n",
    "        \n",
    "    if \"\\t\" not in line:\n",
    "        continue\n",
    "    \n",
    "    encoder_data,decoder_data=line.rstrip().split(\"\\t\")\n",
    "    \n",
    "    temporal_encoder_data.append(encoder_data)\n",
    "    temporal_decoder_data.append(decoder_data)\n",
    "\n",
    "print(\"num samples :\",len(temporal_encoder_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts=[]\n",
    "target_texts_outputs=[]\n",
    "target_texts_inputs=[]\n",
    "\n",
    "indices=np.arange(FETC_EXAMPLES)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "restricted_indices=indices[:NUM_EXAMPLES]\n",
    "\n",
    "for i in range(NUM_EXAMPLES):\n",
    "    \n",
    "    input_texts.append(temporal_encoder_data[restricted_indices[i]])\n",
    "    target_texts_inputs.append(\"<sos> \"+temporal_decoder_data[restricted_indices[i]])\n",
    "    target_texts_outputs.append(temporal_decoder_data[restricted_indices[i]]+\" <eos>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer Encoder Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_inputs=Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences=tokenizer_inputs.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_inputs=tokenizer_inputs.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_input=max(len(i) for i in input_sequences)\n",
    "max_len_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer Decoder Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_outputs=Tokenizer(num_words=MAX_NUM_WORDS,filters=\"\")\n",
    "tokenizer_outputs.fit_on_texts(target_texts_inputs+target_texts_outputs)\n",
    "target_sequence_outputs=tokenizer_outputs.texts_to_sequences(target_texts_outputs)\n",
    "target_sequence_inputs=tokenizer_outputs.texts_to_sequences(target_texts_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_outputs=tokenizer_outputs.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_outputs=len(word2idx_outputs)+1\n",
    "num_words_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_target=max(len(s) for s in target_sequence_outputs)\n",
    "max_len_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pad Sequences Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(encoder_inputs.shape)\n",
    "print(encoder_inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs=pad_sequences(target_sequence_inputs,maxlen=max_len_target,padding=\"post\")\n",
    "decoder_targets=pad_sequences(target_sequence_outputs,maxlen=max_len_target,padding=\"post\")\n",
    "\n",
    "print(decoder_inputs[0])\n",
    "print(decoder_targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Vectors Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors\n",
    "word2vec={}\n",
    "\n",
    "with open(\"glove.6B.50d.txt\") as f:\n",
    "    \n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vec=np.asarray(values[1:],dtype=\"float32\")\n",
    "        word2vec[word]=vec\n",
    "\n",
    "    print(len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "\n",
    "num_words_inputs=min(MAX_NUM_WORDS,len(word2idx_inputs)+1)\n",
    "embedding_matrix=np.zeros((num_words_inputs,EMBEDDING_DIM))\n",
    "\n",
    "for word,i in word2idx_inputs.items():\n",
    "    if i<MAX_NUM_WORDS:\n",
    "        embedding_vector=word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]:embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Decoder Target Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_outputs=min(MAX_NUM_WORDS,len(word2idx_outputs)+1)\n",
    "\n",
    "decoder_targets_one_hot=np.zeros(\n",
    "    (\n",
    "        len(input_texts),\n",
    "        max_len_target,\n",
    "        num_words_outputs\n",
    "    ),\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "\n",
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(decoder_targets):\n",
    "    for t,word in enumerate(d):\n",
    "        decoder_targets_one_hot[i,t,word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer\n",
    "embedding_layer=Embedding(\n",
    "    num_words_inputs,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len_input,\n",
    "    name=\"encoder_embedding\"\n",
    "    #trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder=Input(shape=(max_len_input,),name=\"encoder_inputs\")\n",
    "x=embedding_layer(encoder_inputs_placeholder,)\n",
    "\n",
    "encoder=GRU(LATENT_DIM,return_sequences=True,return_state=True,unroll=True)\n",
    "encoder_outputs,encoder_hidden=encoder(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder=Input(shape=(max_len_target,),name=\"decoder_inputs\")\n",
    "decoder_embedding=Embedding(num_words_outputs,256)\n",
    "decoder_inputs_x=decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "\n",
    "decoder=GRU(LATENT_DIM,return_state=True)\n",
    "decoder_dense=Dense(num_words_outputs,activation=\"softmax\")\n",
    "\n",
    "\n",
    "#\n",
    "dense_w1=Dense(128)\n",
    "dense_w2=Dense(128)\n",
    "dense_v=Dense(1)\n",
    "\n",
    "attn_dot = Dot(axes=1)\n",
    "attention_concat_layer=Concatenate(axis=-1)\n",
    "expand_layer = Lambda(lambda x: K.expand_dims(x, axis=1))\n",
    "attn_repeat_layer = RepeatVector(max_len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(hidden,encoder_outputs):\n",
    "    #print(hidden.shape)\n",
    "    #hidden = attn_repeat_layer(hidden)\n",
    "    score=dense_v(Activation(\"tanh\")(Add()([dense_w1(encoder_outputs),dense_w2(expand_layer(hidden))])))\n",
    "    #print(score)\n",
    "    #attention_weights=K.softmax(score,axis=1)\n",
    "    attention_weights=Activation(\"softmax\")(score)\n",
    "\n",
    "    \n",
    "    c=attn_dot([attention_weights,encoder_outputs])\n",
    "    #print(\"c\",c)\n",
    "    #context_vector=attention_weights*encoder_outputs\n",
    "    #context_vector_reduce_sum=K.sum(context_vector,axis=1)\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first hidden for decoder\n",
    "hidden=encoder_hidden\n",
    "outputs=[]\n",
    "\n",
    "for t in range(max_len_target):   \n",
    "    c=one_step_attention(hidden,encoder_outputs)\n",
    "\n",
    "    selector = Lambda(lambda x: x[:, t:t+1])\n",
    "    xt=selector(decoder_inputs_x)\n",
    "\n",
    "    x=attention_concat_layer([c,xt])\n",
    "\n",
    "    output,state=decoder(x)\n",
    "    pred=decoder_dense(output)\n",
    "\n",
    "    hidden=state\n",
    "    outputs.append(pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_and_transpose(x):\n",
    "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "    x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
    "\n",
    "    x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=[encoder_inputs_placeholder,decoder_inputs_placeholder],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=100,\n",
    "  validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
