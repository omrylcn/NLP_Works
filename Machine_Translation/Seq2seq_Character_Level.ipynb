{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating\n",
    "\n",
    "- It is character-by-character model. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense,GRU\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('tur.txt', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=lines[1221]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sent = []\n",
    "tur_sent = []\n",
    "eng_chars = set()\n",
    "tur_chars = set()\n",
    "nb_samples = 4000\n",
    "\n",
    "# Process english and french sentences\n",
    "for line in range(nb_samples):\n",
    "    \n",
    "    eng_line = str(lines[line]).split('\\t')[0]\n",
    "    \n",
    "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
    "    tur_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
    "    \n",
    "    eng_sent.append(eng_line)\n",
    "    tur_sent.append(tur_line)\n",
    "    \n",
    "    for ch in eng_line:\n",
    "        if (ch not in eng_chars):\n",
    "            eng_chars.add(ch)\n",
    "            \n",
    "    for ch in tur_line:\n",
    "        if (ch not in tur_chars):\n",
    "            tur_chars.add(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 69\n"
     ]
    }
   ],
   "source": [
    "tur_chars = sorted(list(tur_chars))\n",
    "eng_chars = sorted(list(eng_chars))\n",
    "print(len(tur_chars),len(eng_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each english character - key is index and value is english character\n",
    "eng_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get english character given its index - key is english character and value is index\n",
    "eng_char_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(eng_chars):\n",
    "    eng_index_to_char_dict[k] = v\n",
    "    eng_char_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each turkish character - key is index and value is french character\n",
    "tur_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get turkish character given its index - key is turkish character and value is index\n",
    "tur_char_to_index_dict = {}\n",
    "for k, v in enumerate(tur_chars):\n",
    "    tur_index_to_char_dict[k] = v\n",
    "    tur_char_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
    "max_len_tur_sent = max([len(line) for line in tur_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 41)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_eng_sent,max_len_tur_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
    "tokenized_tur_sentences = np.zeros(shape = (nb_samples,max_len_tur_sent,len(tur_chars)), dtype='float32')\n",
    "target_data = np.zeros((nb_samples, max_len_tur_sent, len(tur_chars)),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the english and french sentences\n",
    "\n",
    "for i in range(nb_samples):\n",
    "    #print(eng_sent[i])\n",
    "    for k,ch in enumerate(eng_sent[i]):\n",
    "        #print(i,k,eng_char_to_index_dict[ch],eng_index_to_char_dict[eng_char_to_index_dict[ch]])\n",
    "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
    "        \n",
    "    for k,ch in enumerate(tur_sent[i]):\n",
    "       # print(i,k,tur_char_to_index_dict[ch],tur_index_to_char_dict[tur_char_to_index_dict[ch]])\n",
    "        tokenized_tur_sentences[i,k,tur_char_to_index_dict[ch]] = 1\n",
    "        \n",
    "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "        if k > 0:\n",
    "            target_data[i,k-1,tur_char_to_index_dict[ch]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tur_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tur_sentences[1212][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 13, 69), (4000, 41, 79), (4000, 41, 79))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eng_sentences.shape,tokenized_tur_sentences.shape,target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "\n",
    "encoder_input = Input(shape=(None,len(eng_chars)))\n",
    "#encoder_LSTM = LSTM(256,return_state = True)\n",
    "encoder_outputs=GRU(256,activation=\"relu\",return_sequences=True)(encoder_input)\n",
    "#encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
    "#encoder_states = [encoder_h, encoder_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'gru_4/transpose_1:0' shape=(?, ?, 256) dtype=float32>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "\n",
    "decoder_input = Input(shape=(None,len(tur_chars)))\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(tur_chars),activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, None, 69)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, None, 79)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 256), (None, 333824      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 256),  344064      input_20[0][0]                   \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 79)     20303       lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 698,191\n",
      "Trainable params: 698,191\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=3,verbose=0,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/2000\n",
      "3200/3200 [==============================] - 27s 9ms/step - loss: 1.0926 - val_loss: 1.2703\n",
      "Epoch 2/2000\n",
      "3200/3200 [==============================] - 23s 7ms/step - loss: 0.9893 - val_loss: 1.1819\n",
      "Epoch 3/2000\n",
      "3200/3200 [==============================] - 24s 7ms/step - loss: 0.9109 - val_loss: 1.0699\n",
      "Epoch 4/2000\n",
      "3200/3200 [==============================] - 26s 8ms/step - loss: 0.8209 - val_loss: 0.9613\n",
      "Epoch 5/2000\n",
      "3200/3200 [==============================] - 24s 7ms/step - loss: 0.7534 - val_loss: 0.9031\n",
      "Epoch 6/2000\n",
      "3200/3200 [==============================] - 23s 7ms/step - loss: 0.7004 - val_loss: 0.8582\n",
      "Epoch 7/2000\n",
      "3200/3200 [==============================] - 23s 7ms/step - loss: 0.6643 - val_loss: 0.8420\n",
      "Epoch 8/2000\n",
      "3200/3200 [==============================] - 23s 7ms/step - loss: 0.6362 - val_loss: 0.7811\n",
      "Epoch 9/2000\n",
      "3200/3200 [==============================] - 26s 8ms/step - loss: 0.6117 - val_loss: 0.7706\n",
      "Epoch 10/2000\n",
      "3200/3200 [==============================] - 26s 8ms/step - loss: 0.5908 - val_loss: 0.7422\n",
      "Epoch 11/2000\n",
      "3200/3200 [==============================] - 26s 8ms/step - loss: 0.5712 - val_loss: 0.7275\n",
      "Epoch 12/2000\n",
      "3200/3200 [==============================] - 30s 9ms/step - loss: 0.5543 - val_loss: 0.7119\n",
      "Epoch 13/2000\n",
      "3200/3200 [==============================] - 28s 9ms/step - loss: 0.5385 - val_loss: 0.6963\n",
      "Epoch 14/2000\n",
      "1408/3200 [============>.................] - ETA: 14s - loss: 0.5243"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x=[tokenized_eng_sentences,tokenized_tur_sentences], \n",
    "          y=target_data,\n",
    "          batch_size=64,\n",
    "          epochs=2000,\n",
    "          validation_split=0.2,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(tur_chars)))\n",
    "    target_seq[0, 0, tur_char_to_index_dict['\\t']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_tur_char = tur_index_to_char_dict[max_val_index]\n",
    "        translated_sent += sampled_tur_char\n",
    "        \n",
    "        if ( (sampled_tur_char == '\\n') or (len(translated_sent) > max_len_tur_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(tur_chars)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return translated_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(10):\n",
    "    #seq_index+=40\n",
    "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', eng_sent[seq_index])\n",
    "    print('Decoded sentence:', translated_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
